# Renderer

[[Renderer]] is the core visualization and logical part of
any application. It's attached to the [[Engine]]. Basically,
renders define two methods [[Renderer.load | load()]] and
[[Renderer.update | update()]]. The first one is used to
initialize all assets and prepare the scene e.g. set up
lightning, environment map. The second one is used to update
the scene according to results of video processing. This's
where all the logic happens. Renderers can be extended with
plugins. Plugins do simple rendering task, for example add
object to that follows the head or render avatar overlay.

This package provides set of ready-made renderers and plugins
to simplify development of applications. They can be used as
both atomic building blocks or you can use them as starting
points, inherit and override / extend class functionality.
By extending [[Renderer.load]] and [[Renderer.update]] or
[[ScenePlugin.load]] and [[ScenePlugin.update]] you can add
any custom logic, interactions, animations, post-processing,
effects, gesture recognition, etc.

Module utilizes three.js rendering engine for visualization.
Babylon.js renderers and plugins can be found in
[[@geenee/bodyrenderers-babylon]] package.

# Basics

Set of abstract classes that specialize generic renderers
and plugins for [[PoseProcessor]] and [[FaceProcessor]].
These classes are used as base parents to simplify API,
they do not implement any logic or visualization.

* [[PoseRenderer]] - Abstract renderer for [[PoseProcessor]]
* [[PosePlugin]] - Abstract plugin for [[PoseRenderer]]
* [[FaceRenderer]] - Abstract renderer for [[FaceProcessor]]
* [[FacePlugin]] - Abstract plugin for [[FaceRenderer]]
* [[ThreeRenderer]] - Generic three.js renderer
* [[ThreePlugin]] - Generic plugin for [[ThreeRenderer]]

# Pose tracking

#### [[PoseAlignPlugin]]
Universal plugin aligning node's rig and pose estimated by
[[PoseProcessor]]. It is a base of try-on/outfit, twin, etc.
plugins. You can use this class as starting point and extend
it to customize alignment algorithm or to add more features.
Basically PoseAlignPlugin evaluates positions and rotations
of armature bones based on 3D pose keypoints, then applies
these transforms to bones following the armature hierarchy.
Plugin supports model rigs compatible with Mixamo armature,
e.g. any model from Mixamo library or Ready Player Me avatars.
This is common standard of armature/skeleton for human-like /
anthropomorphic models supported by many game/render engines.
The scene node must contain an armature among its children.
Armature's bones must follow Mixamo / RPM naming convention.
Models rigged and skinned manually or using Mixamo tool can
variate depending on anthropomorphous topology of the model.
For example animated characters can have disproportional body
parts like a much bigger head or longer arms. In such cases
PoseAlignPlugin can apply number of fine-tuning adjustments
to basic alignment improving model fitting or making it look
more natural. [[PoseTuneParams]] explains fine-tuning options.
As an example turning off adjustment of spine curvature gives
better results in virtual garment try-on experiences, while for
full-body avatar overlaying it can provide more natural look.
Depending on the use case and model's topology you can try to
tune different options and see what works better in practice.
By default the plugin is fine-tuned for RPM avatars so you
can simply replace person with the avatar model in the scene.

#### [[PoseOutfitPlugin]]
PoseOutfitPlugin is extension of [[PoseAlignPlugin]] that
allows to specify body meshes of the avatar's scene node
as occluders and optionally hide some child meshes (parts).
It's a good starting point for virtual try-on applications.
[[OutfitParams]] interface defines available outfit options.
You can download any Ready Player Me avatar which outfit
similar to final result, edit its outfit, re-skin model if
necessary. Then simply use this plugin to build try-on app.
Armature bones must follow Mixamo / RPM naming convention.

#### [[PoseTwinPlugin]]
Extension of [[PoseAlignPlugin]] for digital twins mirroring
the pose and residing beside a user. When rendering a twin we
do not translate bones to align with keypoint coordinates and
only preserve relative rotations. After projecting the detected
pose onto a twin, twin's scene node can be further transformed
relative to the initial position - centers of hips are the same.

#### Pose tracking example
* Download pose tracking [example](media://pose-three.zip) for three.js
* Use your personal npm registry key - create `.npmrc` in the root
of the project with `//registry.npmjs.org/:_authToken=YourKey`.
* Run `npm install`.
* Run `npm run start` or `npm run start:https`.
* That's it, you first pose tracking AR application is ready.

#### Preparing models
Guides on preparing models for pose tracking:
* [[models.body | General]]
* [[models.body.automatic | Automatic weights]]
* [[models.body.transfer | Weight transfer]]
* [[models.body.mixamo | Mixamo]]

# Face tracking

#### [[HeadTrackPlugin]]
Plugin attaches provided scene node to the head.
Pose of the node (translation + rotation + scale)
continuously updates according to pose estimation
from [[FaceProcessor]]. All node's children will
hierarchically include this transformation. The
node can be seen as a virtual placeholder for a
real object. It's recommended to attach top-level
nodes that don't include transforms relative to
parent, otherwise head transform that is a pose
in the world frame will be applied on top of them
(will be treated as relative instead of absolute).
Optionally anisotropic fine-tuning of the scale can
be applied. In this case model will additionally
adapt to shape of the face. If face isn't detected
by FaceProcessor plugin recursively hides the node.

Download reference face model: [face.glb](media://face.glb).

Simplified implementation:
```typescript
async update(result: FaceResult, stream: HTMLCanvasElement) {
    if (!this.loaded)
        return;
    const { transform } = result;
    if (!transform) {
        this.node.visible = false;
        return super.update(result, stream);
    }
    // Mesh transformation
    const translation = new three.Vector3(...transform.translation)
    const uniformScale = new three.Vector3().setScalar(transform.scale);
    const shapeScale = new three.Vector3(
        ...transform.shapeScale).multiplyScalar(transform.scale)
    const rotation = new three.Quaternion(...transform.rotation);
    // Align node with the face
    this.node.visible = true;
    this.node.setRotationFromQuaternion(rotation);
    this.node.position.copy(translation);
    this.node.scale.copy(this.shapeScale ? shapeScale : uniformScale);
    // Render
    return super.update(result, stream);
}
```

#### [[FaceTrackPlugin]]
Plugin attaches provided node to the face point.
Pose of the node (translation + rotation + scale)
continuously updates according to pose estimation
from [[FaceProcessor]]. All node's children will
hierarchically include this transformation. The
node can be seen as a virtual placeholder for a
real object. It's recommended to attach top-level
nodes that don't include transforms relative to
parent, otherwise head transform that is a pose
in the world frame will be applied on top of them
(will be treated as relative instead of absolute).
Optionally anisotropic fine-tuning of the scale can
be applied. In this case model will additionally
adapt to shape of the face. If face isn't detected
by FaceProcessor plugin recursively hides the node.

Download reference face model: [face.glb](media://face.glb).

Simplified implementation:
```typescript
async update(result: FaceResult, stream: HTMLCanvasElement) {
    if (!this.loaded)
        return;
    const { transform } = result;
    if (!transform) {
        this.node.visible = false;
        return super.update(result, stream);
    }
    // Mesh transformation
    const translation = new three.Vector3(...transform.translation)
    const uniformScale = new three.Vector3().setScalar(transform.scale);
    const shapeScale = new three.Vector3(
        ...transform.shapeScale).multiplyScalar(transform.scale)
    const rotation = new three.Quaternion(...transform.rotation);
    // Align node with the face
    this.node.visible = true;
    this.node.setRotationFromQuaternion(rotation);
    this.node.position.copy(translation);
    this.node.scale.copy(this.shapeScale ? shapeScale : uniformScale);
    // Render
    return super.update(result, stream);
}
```

#### [[FaceMaskPlugin]]
Adds Mesh object that reflects detected face mesh.
FaceMask creates Mesh and defines indices, uvs and normals
of vertices in [[FaceMask.load | load()]], while positions
are updated in [[FaceMask.update | update()]] according to
face tracking estimations. Plugin uses StandardMaterial
with diffuse texture provided to constructor as image url.

Download face UV map: [faceuv.png](media://faceuv.png)

Simplified implementation:
```typescript
async load(scene?: three.Scene) {
    if (this.loaded || !scene)
        return;
    // Mask geometry
    const geometry = new three.BufferGeometry();
    geometry.setIndex(meshTriangles);
    geometry.setAttribute("position",
        new three.Float32BufferAttribute(new Float32Array(468 * 3), 3));
    geometry.setAttribute("uv",
        new three.Float32BufferAttribute(meshUV.flat(), 2));
    geometry.computeVertexNormals();
    // Texture
    const texture = new three.TextureLoader().load(this.url);
    texture.flipY = false;
    const material = new three.MeshStandardMaterial({
        map: texture, transparent: true
    });
    // Add mask to the scene
    this.mask = new three.Mesh(geometry, material);
    scene.add(this.mask);
    return super.load(scene);
}

async update(result: FaceResult, stream: HTMLCanvasElement) {
    if (!this.loaded)
        return;
    if (!this.mask)
        return super.update(result, stream);
    const { metric } = result;
    if (!metric) {
        this.mask.visible = false;
        return super.update(result, stream);
    }
    // Update mesh coordinates
    this.mask.visible = true;
    let position = this.mask.geometry.getAttribute("position");
    metric.forEach((p, i) =>
        position.setXYZ(i, p[0], p[1], p[2]));
    this.mask.geometry.computeVertexNormals();
    position.needsUpdate = true;
    // Render
    return super.update(result, stream);
}
```

#### Face tracking example
* Download face tracking [example](media://face-three.zip) for three.js
* Use your personal npm registry key - create `.npmrc` in the root
of the project with `//registry.npmjs.org/:_authToken=YourKey`.
* Run `npm install`.
* Run `npm run start` or `npm run start:https`.
* That's it, you first face tracking AR application is ready.

#### Preparing models
Guides on preparing models for face tracking:
* [[models.face | General]]
* [[models.face.head | Head tracking]]
* [[models.face.face | Face tracking]]

# Occluders

#### [[OccluderPlugin]]
Plugin making provided node an occluder. Usually
node is a base mesh (average approximation) of a
body representing its real counterpart in a scene.
Occluders are not rendered by themselves but still
participate in occlusion queries. This is achieved
by setting `colorWrite=false` to all materials of
node's meshes. This flag tells rendering engine to
not write to color buffer but still write to depth
buffer. Then meshes are effectively not rendered
(fragment color write is skipped) and only occlude
all other meshes of the scene (during depth test).

Simplified implementation:
```typescript
async load(scene?: three.Scene) {
    if (this.loaded || !scene)
        return;
    // Occluder material
    this.node.traverse((child) => {
        if (child instanceof three.Mesh)
            child.material.colorWrite = false;
        child.renderOrder = -1;
    });
    return super.load(scene);
}
```
